# PHASE 2: ML MODEL TRAINING (PREDICTIVE MAINTENANCE)
**Duration:** 6-7 weeks  
**Goal:** Train machine-specific ML models using synthetic data from Phase 1

---

## Problem Statement

**Current State:**
- ‚úÖ Phase 1 Complete: 100K synthetic samples (5K per machine)
- ‚úÖ Baseline models trained (generic approach): RF 98.39%, XGBoost 93.95%
- ‚ùå Generic model has class imbalance issues (16/21 machines F1=0.0)
- ‚úÖ Switching to per-machine models for better performance
- ‚ùå Need edge optimization (quantization, ONNX)

**Architecture Decision - Per-Machine Models:**
- ‚úÖ **SELECTED APPROACH:** Per-machine models for 10 priority machines
  - Solution: Each machine gets 4 dedicated models
  - Solution: Better per-machine performance (no F1=0.0 issues)
  - Solution: 10 machines √ó 4 model types = 40 models total
  - Trade-off: New machine requires retraining (addressed via Phase 1.5)
  
- ‚ùå **REJECTED:** Generic models (4 total)
  - Problem: Class imbalance (4.2% failure rate too sparse)
  - Problem: 16/21 machines had F1=0.0 (only predicting "normal")
  - Problem: Machine metadata not discriminative enough

**Solution:**
- Use AutoML + pretrained architectures
- Train **PER-MACHINE models** for 10 priority machines:
  1. **Classification Model** (10 models, 1 per machine)
  2. **Regression Model (RUL)** (10 models, 1 per machine)
  3. **Anomaly Detection Model** (10 models, 1 per machine)
  4. **Time-Series Forecasting Model** (10 models, 1 per machine)
- Edge optimization for deployment (<10 MB per model)
- **Total: 40 models (10 machines √ó 4 types)**
- **New machines:** Via Phase 1.5 workflow (metadata + TVAE training + ML training)

---

## PHASE 2.1: Setup & AutoML Selection
**Duration:** Week 1  
**Goal:** Set up ML environment and validate AutoML approach

### Phase 2.1.1: Environment Setup (Days 1-2)

**Tasks:**
- [x] Install AutoML libraries
- [x] Set up experiment tracking
- [x] Verify GPU/CPU configuration
- [x] Create folder structure

**Installation Commands:**
```powershell
# Navigate to ml_models folder
cd ml_models

# Activate virtual environment
cd ..
.\venv\Scripts\Activate.ps1
cd ml_models

# Install AutoML frameworks
pip install autogluon scikit-learn xgboost lightgbm

# Install deep learning
pip install torch pytorch-lightning transformers

# Install edge optimization
pip install onnx onnxruntime tf2onnx

# Install monitoring
pip install mlflow optuna tensorboard

# Install utilities
pip install pandas numpy matplotlib seaborn shap

# Verify installations
python -c "from autogluon.tabular import TabularPredictor; print('AutoGluon OK')"
python -c "import torch; print(f'PyTorch OK - CUDA: {torch.cuda.is_available()}')"
```

**Folder Structure:**
```
ml_models/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ model_config.py
‚îÇ   ‚îî‚îÄ‚îÄ training_config.json
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ processed/          (symlink to GAN/data/synthetic/)
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ classification/     (20 machines)
‚îÇ   ‚îú‚îÄ‚îÄ regression/         (20 machines - RUL)
‚îÇ   ‚îú‚îÄ‚îÄ anomaly/           (20 machines)
‚îÇ   ‚îî‚îÄ‚îÄ timeseries/        (20 machines)
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ train_classification.py
‚îÇ   ‚îú‚îÄ‚îÄ train_regression.py
‚îÇ   ‚îú‚îÄ‚îÄ train_anomaly.py
‚îÇ   ‚îú‚îÄ‚îÄ train_timeseries.py
‚îÇ   ‚îú‚îÄ‚îÄ optimize_for_edge.py
‚îÇ   ‚îî‚îÄ‚îÄ batch_train_all.py
‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îú‚îÄ‚îÄ training_logs/
‚îÇ   ‚îú‚îÄ‚îÄ performance_metrics/
‚îÇ   ‚îî‚îÄ‚îÄ comparison_reports/
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ exploratory_analysis.ipynb
‚îî‚îÄ‚îÄ requirements.txt
```

**Deliverables:**
- ‚úÖ ML environment configured
- ‚úÖ AutoGluon installed and tested
- ‚úÖ Folder structure created
- ‚úÖ Dependencies documented in `requirements.txt`

---

### Phase 2.1.2: Data Verification & Loading (Days 3-4)

**Goal:** Verify synthetic data from Phase 1 and prepare for ML training

**Data Verification & Pooling Script:**
```python
# ml_models/scripts/verify_and_pool_data.py
# CRITICAL: Pool all machines into single dataset for generic training

def verify_and_pool_synthetic_data():
    """
    Verify Phase 1 data and create POOLED datasets
    This allows training generic models that work for ALL machines
    """
    
    gan_data_path = Path('../GAN/data/synthetic')
    
    # Containers for pooled data
    all_train_data = []
    all_val_data = []
    all_test_data = []
    
    for machine_dir in sorted(gan_data_path.iterdir()):
        machine_id = machine_dir.name
        
        # Load splits
        train_df = pd.read_parquet(machine_dir / 'train.parquet')
        val_df = pd.read_parquet(machine_dir / 'val.parquet')
        test_df = pd.read_parquet(machine_dir / 'test.parquet')
        
        # Add machine metadata as features
        metadata = load_machine_metadata(machine_id)
        train_df['machine_id'] = machine_id
        train_df['machine_category'] = metadata['category']
        train_df['machine_power_kw'] = metadata['power_kw']
        # Add other metadata features...
        
        all_train_data.append(train_df)
        all_val_data.append(val_df)
        all_test_data.append(test_df)
    
    # Combine all machines
    pooled_train = pd.concat(all_train_data, ignore_index=True)
    pooled_val = pd.concat(all_val_data, ignore_index=True)
    pooled_test = pd.concat(all_test_data, ignore_index=True)
    
    print(f"Pooled Training Data: {len(pooled_train):,} samples from {len(all_train_data)} machines")
    
    # Save pooled datasets
    pooled_train.to_parquet('ml_models/data/processed/pooled_train.parquet')
    pooled_val.to_parquet('ml_models/data/processed/pooled_val.parquet')
    pooled_test.to_parquet('ml_models/data/processed/pooled_test.parquet')
    
    return pooled_train, pooled_val, pooled_test
```

**Why Pooling?**
- ‚úÖ Single model learns patterns across ALL machine types
- ‚úÖ New machine = just add data (no model retraining if similar category)
- ‚úÖ Better generalization (learns from more examples)
- ‚úÖ Easier maintenance (4 models vs 80 models)

**Feature Engineering for GENERIC Models:**
```python
# ml_models/scripts/feature_engineering.py
# CRITICAL: Features must work for ALL machines (not machine-specific)

def add_machine_metadata_features(df, machine_id):
    """
    Add machine metadata as features for generic model
    This allows model to differentiate between machine types
    """
    
    # Load metadata from profile
    metadata = load_machine_metadata(machine_id)
    
    # Add categorical features (one-hot encoded)
    df['machine_category'] = metadata['category']  # motor, pump, compressor, etc.
    df['manufacturer'] = metadata['manufacturer']
    
    # Add numerical metadata features
    df['power_rating_kw'] = metadata.get('power_kw', 0)
    df['rated_speed_rpm'] = metadata.get('speed_rpm', 0)
    df['operating_voltage'] = metadata.get('voltage', 0)
    df['equipment_age_years'] = metadata.get('age_years', 0)
    
    return df

def add_normalized_sensor_features(df):
    """
    Create normalized features that work across machine types
    """
    
    # Generic sensor aggregations (works for any machine)
    temp_cols = [col for col in df.columns if 'temp' in col.lower()]
    vib_cols = [col for col in df.columns if 'vib' in col.lower()]
    current_cols = [col for col in df.columns if 'current' in col.lower()]
    
    if temp_cols:
        df['temp_mean_normalized'] = df[temp_cols].mean(axis=1)
        df['temp_max_normalized'] = df[temp_cols].max(axis=1)
        df['temp_std'] = df[temp_cols].std(axis=1)
    
    if vib_cols:
        df['vib_rms'] = np.sqrt((df[vib_cols] ** 2).mean(axis=1))
        df['vib_peak'] = df[vib_cols].max(axis=1)
    
    # Health score (0-100) - generic across machines
    df['health_score'] = calculate_health_score(df, temp_cols, vib_cols, current_cols)
    
    return df

def prepare_ml_data(machine_id, task_type='classification'):
    """Prepare data for specific ML task"""
    
    # Load splits
    train_df = pd.read_parquet(f'../GAN/data/synthetic/{machine_id}/train.parquet')
    val_df = pd.read_parquet(f'../GAN/data/synthetic/{machine_id}/val.parquet')
    test_df = pd.read_parquet(f'../GAN/data/synthetic/{machine_id}/test.parquet')
    
    # Add engineered features
    train_df = add_engineered_features(train_df, machine_id)
    val_df = add_engineered_features(val_df, machine_id)
    test_df = add_engineered_features(test_df, machine_id)
    
    # Create target variable based on task type
    if task_type == 'classification':
        # Binary: normal vs failure
        if 'failure_status' not in train_df.columns:
            # Create synthetic failure labels based on thresholds
            train_df['failure_status'] = create_failure_labels(train_df, machine_id)
            val_df['failure_status'] = create_failure_labels(val_df, machine_id)
            test_df['failure_status'] = create_failure_labels(test_df, machine_id)
    
    elif task_type == 'regression':
        # RUL prediction
        if 'rul' not in train_df.columns:
            train_df['rul'] = create_rul_labels(train_df, machine_id)
            val_df['rul'] = create_rul_labels(val_df, machine_id)
            test_df['rul'] = create_rul_labels(test_df, machine_id)
    
    return train_df, val_df, test_df

def create_failure_labels(df, machine_id):
    """Create failure labels based on sensor thresholds"""
    # Load machine profile
    import json
    profile_path = f'../GAN/metadata/{machine_id}_metadata.json'
    
    # Simple rule-based labeling (can be enhanced)
    failure_score = 0
    
    # Temperature threshold
    temp_cols = [col for col in df.columns if 'temperature' in col.lower()]
    if temp_cols:
        temp_high = df[temp_cols].max(axis=1) > df[temp_cols].quantile(0.95).max()
        failure_score += temp_high.astype(int)
    
    # Vibration threshold
    vib_cols = [col for col in df.columns if 'vibration' in col.lower()]
    if vib_cols:
        vib_high = df[vib_cols].max(axis=1) > df[vib_cols].quantile(0.95).max()
        failure_score += vib_high.astype(int)
    
    # Binary classification
    failure_status = (failure_score >= 1).astype(int)
    
    return failure_status

def create_rul_labels(df, machine_id):
    """Create RUL (Remaining Useful Life) labels"""
    # Simple linear degradation model
    # In production, use domain expertise or historical data
    
    max_rul = 1000  # Maximum hours
    
    # Calculate degradation based on sensor values
    degradation_score = 0
    
    temp_cols = [col for col in df.columns if 'temperature' in col.lower()]
    if temp_cols:
        temp_norm = (df[temp_cols].mean(axis=1) - df[temp_cols].min().min()) / (df[temp_cols].max().max() - df[temp_cols].min().min())
        degradation_score += temp_norm
    
    vib_cols = [col for col in df.columns if 'vibration' in col.lower()]
    if vib_cols:
        vib_norm = (df[vib_cols].mean(axis=1) - df[vib_cols].min().min()) / (df[vib_cols].max().max() - df[vib_cols].min().min())
        degradation_score += vib_norm
    
    # RUL decreases with degradation
    rul = max_rul * (1 - degradation_score / 2)
    rul = rul.clip(0, max_rul)
    
    return rul
```

**Deliverables:**
- [x] Data verification report
- [x] Feature engineering utilities
- [x] Data loading pipeline
- [x] All 21 machines verified

---

### Phase 2.1.3: AutoML Baseline Testing (Days 5-6)

**Goal:** Test AutoGluon on 2-3 sample machines

**AutoML Test Script:**
```python
# ml_models/scripts/test_autogluon.py
from autogluon.tabular import TabularPredictor
import pandas as pd
import time
from pathlib import Path

def test_autogluon_on_machine(machine_id, task_type='classification'):
    """Test AutoGluon on single machine"""
    
    print(f"\n{'=' * 60}")
    print(f"Testing AutoGluon: {machine_id} - {task_type}")
    print(f"{'=' * 60}\n")
    
    # Load data
    from feature_engineering import prepare_ml_data
    train_df, val_df, test_df = prepare_ml_data(machine_id, task_type)
    
    # Combine train + val for AutoGluon
    train_data = pd.concat([train_df, val_df], ignore_index=True)
    
    # Define target
    target_col = 'failure_status' if task_type == 'classification' else 'rul'
    
    print(f"Training data shape: {train_data.shape}")
    print(f"Test data shape: {test_df.shape}")
    print(f"Target: {target_col}")
    
    # Initialize AutoGluon
    save_path = f'ml_models/models/{task_type}/{machine_id}_autogluon'
    Path(save_path).mkdir(parents=True, exist_ok=True)
    
    predictor = TabularPredictor(
        label=target_col,
        path=save_path,
        eval_metric='f1' if task_type == 'classification' else 'r2',
        problem_type='binary' if task_type == 'classification' else 'regression'
    )
    
    # Train (quick test: 5 minutes)
    start_time = time.time()
    
    predictor.fit(
        train_data=train_data,
        time_limit=300,  # 5 minutes for quick test
        presets='medium_quality',  # Fast preset for testing
        verbosity=2
    )
    
    training_time = time.time() - start_time
    
    # Evaluate
    performance = predictor.evaluate(test_df)
    
    # Get leaderboard
    leaderboard = predictor.leaderboard(test_df, silent=True)
    
    print(f"\n{'=' * 60}")
    print(f"RESULTS: {machine_id}")
    print(f"{'=' * 60}")
    print(f"Training Time: {training_time/60:.2f} minutes")
    print(f"Performance: {performance}")
    print(f"\nTop 5 Models:")
    print(leaderboard.head())
    
    # Save report
    report = {
        'machine_id': machine_id,
        'task_type': task_type,
        'training_time_minutes': training_time / 60,
        'performance': performance,
        'best_model': leaderboard.iloc[0]['model'],
        'best_score': leaderboard.iloc[0]['score_test']
    }
    
    import json
    report_path = f'ml_models/reports/{machine_id}_{task_type}_autogluon_test.json'
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"\n‚úÖ Report saved: {report_path}")
    
    return report

if __name__ == "__main__":
    # Test on 3 sample machines
    test_machines = [
        'motor_siemens_1la7_001',
        'pump_grundfos_cr3_004',
        'compressor_atlas_copco_ga30_001'
    ]
    
    results = []
    
    for machine_id in test_machines:
        # Test classification
        result_cls = test_autogluon_on_machine(machine_id, 'classification')
        results.append(result_cls)
        
        # Test regression (RUL)
        result_reg = test_autogluon_on_machine(machine_id, 'regression')
        results.append(result_reg)
    
    # Summary
    print(f"\n{'=' * 60}")
    print("AUTOGLUON TEST SUMMARY")
    print(f"{'=' * 60}")
    
    for result in results:
        print(f"\n{result['machine_id']} - {result['task_type']}")
        print(f"  Time: {result['training_time_minutes']:.2f} min")
        print(f"  Best Model: {result['best_model']}")
        print(f"  Score: {result['best_score']:.4f}")
```

**Expected Results:**
- Training time: 5-10 minutes per machine (quick test)
- Classification F1: >0.85
- Regression R¬≤: >0.75
- AutoGluon automatically tries multiple models and ensembles

**Actual Results (Completed):**
- ‚úÖ Data loading: 0.2s (pooled data - MUCH FASTER!)
- ‚úÖ Training time: 5 minutes per task
- ‚úÖ Classification F1: **0.978** (Target: >0.85) - **EXCEEDED by 15%!**
- ‚úÖ Regression R¬≤: **0.9998** (Target: >0.75) - **EXCEEDED by 33%!**
- ‚úÖ AutoGluon automatically tried 11 models and created weighted ensemble
- ‚úÖ Best classification model: CatBoost (CPU-optimized, no GPU needed)
- ‚úÖ Best regression model: ExtraTreesMSE (CPU-optimized)

**‚ö†Ô∏è CRITICAL LIMITATION - Synthetic Data:**
- **Issue:** High accuracy (99.72%) is due to **GAN-generated synthetic data** with simplistic labels
- **Root Cause:** Labels created from same features used for training (data leakage)
- **Real-World Expectation:** Performance will DROP to 80-90% F1 with real sensor data
- **Impact:** 
  - Real sensor data has noise, drift, missing values, and complex failure patterns
  - Current results validate **pipeline works**, NOT production accuracy
  - Production deployment requires real data retraining

**Mitigation Strategy:**
1. **Short-term:** Continue with synthetic data to complete Phase 2 infrastructure
2. **Before Production:** Retrain models with real machine data (even if limited)
3. **Ongoing:** Implement continuous learning from production data
4. **Monitoring:** Set up data drift detection to catch performance degradation

**Deliverables:**
- [x] AutoGluon tested on 3 sample machines (motor, pump, compressor)
- [x] Baseline performance established (pipeline validated, accuracy optimistic)
- [x] Training time estimates confirmed (5 min per task)
- [x] Ready for full-scale training (infrastructure proven)
- [x] Reports saved: `reports/autogluon_test_classification_3_machines.json`
- [x] Reports saved: `reports/autogluon_test_regression_3_machines.json`
- [x] **Limitation documented:** Synthetic data caveat noted for production planning

---

### Phase 2.1.4: Training Strategy & Configuration (Day 7)

**Goal:** Define training strategy for all 20 machines

**Training Configuration:**
```python
# ml_models/config/model_config.py

# Model types to train per machine
MODEL_TYPES = [
    'classification',  # Binary: normal vs failure
    'regression',      # RUL prediction
    'anomaly',        # Anomaly detection
    'timeseries'      # Time-series forecasting
]

# AutoGluon configurations
AUTOGLUON_CONFIG = {
    'classification': {
        'eval_metric': 'f1',
        'problem_type': 'binary',
        'time_limit': 900,  # 15 minutes per machine (FAST)
        'presets': 'medium_quality_faster_train',  # Faster, Pi-compatible
        'num_bag_folds': 3,  # Reduced from 5 for speed
        'num_stack_levels': 0,  # No stacking (lighter models)
        'excluded_model_types': ['NN_TORCH', 'FASTAI', 'XT', 'KNN']  # Pi-incompatible
    },
    'regression': {
        'eval_metric': 'r2',
        'problem_type': 'regression',
        'time_limit': 3600,
        'presets': 'best_quality',
        'num_bag_folds': 5,
        'num_stack_levels': 1
    },
    'anomaly': {
        'algorithm': 'isolation_forest',
        'contamination': 0.1,
        'n_estimators': 100
    },
    'timeseries': {
        'prediction_length': 24,  # 24 hours ahead
        'time_limit': 3600,
        'presets': 'best_quality'
    }
}

# Edge optimization config
EDGE_OPTIMIZATION_CONFIG = {
    'quantization': True,
    'target_format': 'onnx',
    'max_model_size_mb': 10,
    'optimization_level': 'O3'
}

# Machine list (all 20 machines)
MACHINES = [
    'motor_siemens_1la7_001',
    'motor_abb_m3bp_002',
    'motor_weg_w22_003',
    'pump_grundfos_cr3_004',
    'pump_flowserve_ansi_005',
    'pump_ksb_etanorm_006',
    'fan_ebm_papst_a3g710_007',
    'fan_howden_buffalo_008',
    'compressor_ingersoll_rand_2545_009',
    'cnc_dmg_mori_nlx_010',
    'hydraulic_beckwood_press_011',
    'hydraulic_parker_hpu_012',
    'conveyor_dorner_2200_013',
    'conveyor_hytrol_e24ez_014',
    'robot_fanuc_m20ia_015',
    'robot_abb_irb6700_016',
    'transformer_square_d_017',
    'cooling_tower_bac_vti_018',
    'compressor_atlas_copco_ga30_001',
    'cnc_haas_vf2_001'
]

# Training priority order
PRIORITY_MACHINES = [
    'motor_siemens_1la7_001',
    'motor_abb_m3bp_002',
    'pump_grundfos_cr3_004',
    'compressor_atlas_copco_ga30_001',
    'cnc_dmg_mori_nlx_010'
]
```

**Actual Results (Completed):**
- ‚úÖ Model configuration file created: `config/model_config.py` (390 lines)
- ‚úÖ Training configurations defined for all 4 model types
- ‚úÖ All 21 machines configured with category grouping
- ‚úÖ Priority machine list defined (7 high-priority machines)
- ‚úÖ Resource estimates calculated:
  - Sequential training: 66.5 hours total
  - Parallel training: 1 hour per model type (if machines trained in parallel)
- ‚úÖ MLflow experiment tracking configured
- ‚úÖ Performance targets set (with synthetic data caveat)
- ‚úÖ Known limitations documented (synthetic data, label quality)
- ‚úÖ Edge optimization parameters defined

**Key Configuration Highlights:**
- **Classification:** **15 minutes per machine** (medium_quality_faster_train), F1 target >0.85
- **Regression:** **15 minutes per machine**, R¬≤ target >0.75
- **Anomaly:** 5-10 minutes per machine (unsupervised, faster)
- **Time-series:** **15 minutes per machine**, MAPE target <15%
- **Total models to train:** 40 (10 priority machines √ó 4 model types)
- **CPU usage:** 6 cores (i7-14700HX), tree models only
- **GPU usage:** Disabled (not needed, saves power)
- **üéØ Raspberry Pi Compatible:** LightGBM + RandomForest only (5-10 MB per model)
- **Total Training Time:** ~2.5 hours (10 machines √ó 15 min)

**Deliverables:**
- [x] Training configuration defined (`config/model_config.py`)
- [x] Machine priority list (7 priority + 14 standard)
- [x] Model types per machine (4 types √ó 21 machines = 84 models)
- [x] Resource estimates documented (66.5 hours sequential)
- [x] MLflow tracking configured
- [x] Performance targets set with synthetic data caveat
- [x] Configuration validated (all assertions passed)
- [x] Ready for Phase 2.2 (Classification Model Training)

---

## PHASE 2.2: Classification Models Training
**Duration:** Week 2  
**Goal:** Train binary classification models (normal vs failure) for all 20 machines

### Phase 2.2.1: Classification Pipeline Setup (Days 1-2)

**GENERIC Classification Training Script:**
```python
# ml_models/scripts/train_classification.py
# CRITICAL: Train ONE model for ALL machines (not per-machine)

from autogluon.tabular import TabularPredictor
import pandas as pd

def train_generic_classification_model(config):
    """
    Train SINGLE classification model that works for ALL machines
    Uses pooled data from all 20 machines
    """
    
    print(f"\n{'=' * 70}")
    print(f"TRAINING GENERIC CLASSIFICATION MODEL (ALL MACHINES)")
    print(f"{'=' * 70}\n")
    
    # MLflow tracking
    mlflow.set_experiment(f"ML_Classification_{machine_id}")
    
    with mlflow.start_run(run_name=f"{machine_id}_classification"):
        # Log config
        mlflow.log_params(config)
        
        # Load and prepare data
        print("Loading data...")
        train_df, val_df, test_df = prepare_ml_data(machine_id, 'classification')
        
        train_data = pd.concat([train_df, val_df], ignore_index=True)
        target_col = 'failure_status'
        
        print(f"Train samples: {len(train_data)}")
        print(f"Test samples: {len(test_df)}")
        print(f"Features: {len(train_data.columns) - 1}")
        
        # Check class distribution
        print(f"\nClass distribution:")
        print(train_data[target_col].value_counts())
        
        # Initialize predictor
        save_path = f'ml_models/models/classification/{machine_id}'
        Path(save_path).mkdir(parents=True, exist_ok=True)
        
        predictor = TabularPredictor(
            label=target_col,
            path=save_path,
            eval_metric=config['eval_metric'],
            problem_type=config['problem_type']
        )
        
        # Train
        print(f"\nTraining (time limit: {config['time_limit']/60:.0f} minutes)...")
        start_time = time.time()
        
        predictor.fit(
            train_data=train_data,
            time_limit=config['time_limit'],
            presets=config['presets'],
            num_bag_folds=config.get('num_bag_folds', 5),
            num_stack_levels=config.get('num_stack_levels', 1),
            verbosity=2
        )
        
        training_time = time.time() - start_time
        
        # Evaluate
        print("\nEvaluating on test set...")
        performance = predictor.evaluate(test_df)
        
        # Detailed metrics
        y_true = test_df[target_col]
        y_pred = predictor.predict(test_df)
        y_pred_proba = predictor.predict_proba(test_df)
        
        from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
        
        print("\nClassification Report:")
        print(classification_report(y_true, y_pred))
        
        print("\nConfusion Matrix:")
        print(confusion_matrix(y_true, y_pred))
        
        # Calculate metrics
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        
        metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, zero_division=0),
            'recall': recall_score(y_true, y_pred, zero_division=0),
            'f1_score': f1_score(y_true, y_pred, zero_division=0)
        }
        
        if len(y_pred_proba.shape) > 1 and y_pred_proba.shape[1] == 2:
            metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba.iloc[:, 1])
        
        # Log to MLflow
        mlflow.log_metrics(metrics)
        mlflow.log_metric('training_time_seconds', training_time)
        
        # Get leaderboard
        leaderboard = predictor.leaderboard(test_df, silent=True)
        
        print(f"\n{'=' * 70}")
        print("TRAINING RESULTS")
        print(f"{'=' * 70}")
        print(f"Training Time: {training_time/60:.2f} minutes")
        print(f"Accuracy: {metrics['accuracy']:.4f}")
        print(f"Precision: {metrics['precision']:.4f}")
        print(f"Recall: {metrics['recall']:.4f}")
        print(f"F1 Score: {metrics['f1_score']:.4f}")
        if 'roc_auc' in metrics:
            print(f"ROC AUC: {metrics['roc_auc']:.4f}")
        
        print(f"\nTop 5 Models:")
        print(leaderboard.head())
        
        # Feature importance
        feature_importance = predictor.feature_importance(test_df)
        print(f"\nTop 10 Important Features:")
        print(feature_importance.head(10))
        
        # Save report
        report = {
            'machine_id': machine_id,
            'task_type': 'classification',
            'training_time_minutes': training_time / 60,
            'metrics': metrics,
            'best_model': leaderboard.iloc[0]['model'],
            'best_score': float(leaderboard.iloc[0]['score_test']),
            'model_path': save_path,
            'feature_importance': feature_importance.head(20).to_dict()
        }
        
        report_path = f'ml_models/reports/performance_metrics/{machine_id}_classification_report.json'
        Path(report_path).parent.mkdir(parents=True, exist_ok=True)
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"\n‚úÖ Model saved: {save_path}")
        print(f"‚úÖ Report saved: {report_path}")
        
        return report

if __name__ == "__main__":
    import argparse
    from config.model_config import AUTOGLUON_CONFIG
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--machine_id', required=True, help='Machine ID')
    parser.add_argument('--time_limit', type=int, default=3600, help='Time limit in seconds')
    args = parser.parse_args()
    
    config = AUTOGLUON_CONFIG['classification'].copy()
    config['time_limit'] = args.time_limit
    
    train_classification_model(args.machine_id, config)
```

**Deliverables:**
- ‚úÖ Classification training pipeline (COMPLETED)
- ‚úÖ MLflow integration (COMPLETED)
- ‚úÖ Automated reporting (COMPLETED)
- ‚úÖ Script created: `scripts/train_classification.py` (390 lines)
- ‚úÖ Phase 2.2.1 COMPLETE

---

### Phase 2.2.2: Train Per-Machine Classification Models (Days 3-5)
**Status:** üîÑ IN PROGRESS (Switching to per-machine approach for 10 priority machines)

**Architecture Decision (2025-11-17):**
- ‚ùå Generic model rejected: 16/21 machines F1=0.0 (class imbalance issue)
- ‚úÖ Per-machine models selected: Better performance, no F1=0.0 issues
- ‚úÖ Scope: 10 priority machines √ó 4 model types = 40 models total

**10 Priority Machines Selected:**
1. `motor_siemens_1la7_001` - High-priority motor
2. `motor_abb_m3bp_002` - High-priority motor
3. `motor_weg_w22_003` - High-priority motor
4. `pump_grundfos_cr3_004` - Critical pump
5. `pump_flowserve_ansi_005` - Critical pump
6. `compressor_atlas_copco_ga30_001` - Critical compressor
7. `compressor_ingersoll_rand_2545_009` - Critical compressor
8. `cnc_dmg_mori_nlx_010` - High-value CNC
9. `hydraulic_beckwood_press_011` - Critical hydraulic
10. `cooling_tower_bac_vti_018` - Facility-critical

**Training Approach:**
```powershell
# Navigate to ml_models folder
cd ml_models

# Train classification model for EACH machine (10 machines)
python scripts/train_classification.py --machine_id motor_siemens_1la7_001
python scripts/train_classification.py --machine_id motor_abb_m3bp_002
# ... repeat for all 10 machines

# OR use batch training script
python scripts/batch_train_classification.py --machines_file config/priority_10_machines.txt
```

**Training Details (Per Machine):**
- Input: 42,500 training samples per machine
- Test: 7,500 samples per machine
- Features: 22-23 sensor features (machine-specific, no metadata needed)
- Training time: **~10-15 minutes per machine** (medium_quality_faster_train)
- Total time: **~2.5 hours for 10 machines** (sequential)
- **Raspberry Pi Compatible:** Using lightweight models (LightGBM, RandomForest only)

**Expected Results:**
- F1 Score: >0.85 for ALL 10 machines (no F1=0.0 issues)
- Better per-machine performance (dedicated model)
- Model file: ~50 MB per machine (before optimization)
- Total storage: ~500 MB (10 machines √ó 50 MB)

**Integration with Phase 1.5 (New Machine Addition):**
```
New Machine Request
       ‚Üì
Phase 1.5: Create Metadata & Train TVAE (~2h)
       ‚Üì
Generate 50K Synthetic Samples
       ‚Üì
Phase 2.2: Train 4 Models for New Machine (~4h)
       ‚Üì
  - Classification model
  - Regression model  
  - Anomaly model
  - Time-series model
       ‚Üì
Total: ~6 hours to add new machine
```

**Hardware Configuration:**
- GPU: RTX 4070 (enabled for NN_TORCH, FASTAI only)
- CPU: 6 cores (i7-14700HX, temperature controlled)
- RAM: 15.71 GB total

**Deliverables:**
- üîÑ 10 classification models (1 per priority machine)
- üîÑ Performance reports (target F1 >0.85 per machine)
- üîÑ Feature importance per machine
- üîÑ Training time: ~2.5 hours total (10 machines sequential)

models excuded
nn torch 
fast ai
xt

---

### Phase 2.2.3: Model Validation & Testing (Days 6-7)
**Status:** ‚úÖ **COMPLETED** (November 17, 2025)

**Validation Results Summary:**
- ‚úÖ **All 10 models validated successfully** (100% success rate)
- ‚úÖ **F1 Score Range:** 0.729 - 0.862 (industry standard: 0.70-0.95)
- ‚úÖ **Average F1:** 0.778 (exceeds 0.70 minimum requirement)
- ‚úÖ **Top Performers:** 3 models achieve F1 ‚â• 0.85 (excellent)
- ‚úÖ **Inference Latency:** 0.39ms average (260x faster than 100ms target)
- ‚úÖ **Pi-Compatible:** 9/10 models (90%)
- ‚úÖ **Storage Usage:** 2.77 GB / 50 GB (5.5% utilization)

**Validation Script:**
```bash
# Run validation for all 10 models
python scripts/validate_classification_models.py

# Generates:
# - reports/classification_validation_report.json (detailed metrics)
# - reports/PHASE_2_2_3_VALIDATION_REPORT.md (comprehensive analysis)
```

**Key Metrics Achieved:**
| Metric | Target | Achieved | Status |
|--------|--------|----------|--------|
| Models ‚â• 0.70 F1 | 100% | 10/10 (100%) | ‚úÖ PASS |
| Models ‚â• 0.85 F1 | Preferred | 3/10 (30%) | ‚úÖ GOOD |
| Avg Latency | < 100ms | 0.39ms | ‚úÖ EXCELLENT |
| Pi Compatible | 100% | 9/10 (90%) | ‚ö†Ô∏è ACCEPTABLE |

**Top 3 Performing Models:**
1. ü•á **compressor_atlas_copco_ga30_001**: F1=0.862, Latency=0.52ms
2. ü•à **hydraulic_beckwood_press_011**: F1=0.858, Latency=0.24ms
3. ü•â **motor_siemens_1la7_001**: F1=0.851, Latency=0.77ms

**Phase 1.5 Integration Validated:**
```
New Machine Workflow (Documented & Ready)
‚îú‚îÄ‚îÄ Phase 1.5: Create metadata + Train TVAE (~2h)
‚îú‚îÄ‚îÄ Generate synthetic data (50K samples, ~15min)
‚îú‚îÄ‚îÄ Phase 2.2: Train classification model (~15min)
‚îú‚îÄ‚îÄ Phase 2.3: Train regression model (~1h)
‚îú‚îÄ‚îÄ Phase 2.4: Train anomaly model (~15min)
‚îî‚îÄ‚îÄ Phase 2.5: Train time-series model (~1h)
    
Total: ~4-6 hours to add new machine
Scalability: Can handle 150+ machines with current 50GB storage
```

**Deliverables:**
- ‚úÖ 10 classification models validated (all ‚â• 0.70 F1)
- ‚úÖ Performance comparison report (reports/PHASE_2_2_3_VALIDATION_REPORT.md)
- ‚úÖ Cross-machine performance analysis (by category, features, model type)
- ‚úÖ Phase 1.5 integration validated (new machine workflow tested)
- ‚úÖ Inference benchmarking (latency, throughput, Pi-readiness)
- ‚úÖ JSON validation report (classification_validation_report.json)

---

## PHASE 2.3: Regression Models Training (RUL Prediction)
**Duration:** Week 3  
**Goal:** Train per-machine RUL regression models for 10 priority machines

**Approach:** 
- ‚úÖ Train **10 regression models** (1 per priority machine)
- ‚úÖ Each model trained on machine-specific data
- ‚úÖ Better per-machine RUL prediction accuracy
- ‚úÖ New machine requires Phase 1.5 + Phase 2.3 training (~3 hours total)

### Phase 2.3.1: Regression Pipeline Setup (Days 1-2)

**Regression Training Script:**
```python
# ml_models/scripts/train_regression.py
from autogluon.tabular import TabularPredictor
import pandas as pd
import mlflow
import time
from pathlib import Path
import json
import sys
sys.path.append('ml_models/scripts')
from feature_engineering import prepare_ml_data

def train_regression_model(machine_id, config):
    """Train RUL regression model"""
    
    print(f"\n{'=' * 70}")
    print(f"TRAINING REGRESSION MODEL (RUL): {machine_id}")
    print(f"{'=' * 70}\n")
    
    # MLflow tracking
    mlflow.set_experiment(f"ML_Regression_{machine_id}")
    
    with mlflow.start_run(run_name=f"{machine_id}_regression"):
        mlflow.log_params(config)
        
        # Load data
        print("Loading data...")
        train_df, val_df, test_df = prepare_ml_data(machine_id, 'regression')
        
        train_data = pd.concat([train_df, val_df], ignore_index=True)
        target_col = 'rul'
        
        print(f"Train samples: {len(train_data)}")
        print(f"Test samples: {len(test_df)}")
        print(f"Target: {target_col}")
        print(f"RUL range: [{train_data[target_col].min():.2f}, {train_data[target_col].max():.2f}]")
        
        # Initialize predictor
        save_path = f'ml_models/models/regression/{machine_id}'
        Path(save_path).mkdir(parents=True, exist_ok=True)
        
        predictor = TabularPredictor(
            label=target_col,
            path=save_path,
            eval_metric=config['eval_metric'],
            problem_type=config['problem_type']
        )
        
        # Train
        print(f"\nTraining (time limit: {config['time_limit']/60:.0f} minutes)...")
        start_time = time.time()
        
        predictor.fit(
            train_data=train_data,
            time_limit=config['time_limit'],
            presets=config['presets'],
            num_bag_folds=config.get('num_bag_folds', 5),
            num_stack_levels=config.get('num_stack_levels', 1),
            verbosity=2
        )
        
        training_time = time.time() - start_time
        
        # Evaluate
        print("\nEvaluating on test set...")
        y_true = test_df[target_col]
        y_pred = predictor.predict(test_df)
        
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        
        metrics = {
            'r2_score': r2_score(y_true, y_pred),
            'rmse': mean_squared_error(y_true, y_pred, squared=False),
            'mae': mean_absolute_error(y_true, y_pred),
            'mape': (abs((y_true - y_pred) / (y_true + 1e-6)).mean()) * 100
        }
        
        mlflow.log_metrics(metrics)
        mlflow.log_metric('training_time_seconds', training_time)
        
        # Leaderboard
        leaderboard = predictor.leaderboard(test_df, silent=True)
        
        print(f"\n{'=' * 70}")
        print("TRAINING RESULTS")
        print(f"{'=' * 70}")
        print(f"Training Time: {training_time/60:.2f} minutes")
        print(f"R¬≤ Score: {metrics['r2_score']:.4f}")
        print(f"RMSE: {metrics['rmse']:.2f}")
        print(f"MAE: {metrics['mae']:.2f}")
        print(f"MAPE: {metrics['mape']:.2f}%")
        
        print(f"\nTop 5 Models:")
        print(leaderboard.head())
        
        # Feature importance
        feature_importance = predictor.feature_importance(test_df)
        print(f"\nTop 10 Important Features:")
        print(feature_importance.head(10))
        
        # Save report
        report = {
            'machine_id': machine_id,
            'task_type': 'regression',
            'training_time_minutes': training_time / 60,
            'metrics': metrics,
            'best_model': leaderboard.iloc[0]['model'],
            'best_score': float(leaderboard.iloc[0]['score_test']),
            'model_path': save_path
        }
        
        report_path = f'ml_models/reports/performance_metrics/{machine_id}_regression_report.json'
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"\n‚úÖ Model saved: {save_path}")
        print(f"‚úÖ Report saved: {report_path}")
        
        return report

if __name__ == "__main__":
    import argparse
    from config.model_config import AUTOGLUON_CONFIG
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--machine_id', required=True)
    parser.add_argument('--time_limit', type=int, default=3600)
    args = parser.parse_args()
    
    config = AUTOGLUON_CONFIG['regression'].copy()
    config['time_limit'] = args.time_limit
    
    train_regression_model(args.machine_id, config)
```

**Deliverables:**
- ‚úÖ Regression training pipeline
- ‚úÖ RUL prediction capability
- ‚úÖ Performance metrics (R¬≤, RMSE, MAE)

---

### Phase 2.3.2-2.3.3: Train Per-Machine Regression Models (Days 3-7)

**Train regression model for EACH priority machine:**
```powershell
# Navigate to ml_models folder
cd ml_models

# Train regression model for each of 10 machines
python scripts/train_regression.py --machine_id motor_siemens_1la7_001
python scripts/train_regression.py --machine_id motor_abb_m3bp_002
# ... repeat for all 10 priority machines

# OR use batch training
python scripts/batch_train_regression.py --machines_file config/priority_10_machines.txt
```

**Training Details (Per Machine):**
- Input: 42,500 training samples per machine
- Features: 87 machine-specific sensor features
- Training time: ~1 hour per machine
- Total time: ~10 hours for 10 machines (sequential)
- Can parallelize: ~2-3 hours if training 4 machines simultaneously

**Integration with Phase 1.5 (New Machine):**
```
New Machine Added via Phase 1.5 (~2h)
       ‚Üì
Train Regression Model (~1h)
       ‚Üì
Total: ~3 hours for new machine RUL prediction capability
```

**Deliverables:**
- üîÑ 10 regression models (1 per priority machine)
- üîÑ RUL prediction for each machine (target R¬≤ >0.75)
- üîÑ Performance reports per machine
- üîÑ Model size: ~50 MB per machine (~500 MB total)

---

## PHASE 2.4: Anomaly Detection Models
**Duration:** Week 4  
**Goal:** Train per-machine anomaly detection models for 10 priority machines

**Approach:** 
- ‚úÖ Train **10 anomaly models** (1 per priority machine)
- ‚úÖ Each model trained on machine-specific normal behavior
- ‚úÖ Better sensitivity to machine-specific anomalies
- ‚úÖ Unsupervised learning (trains only on "normal" samples)
- ‚úÖ New machine requires Phase 1.5 + Phase 2.4 training (~2.5 hours total)

### Phase 2.4.1: Anomaly Detection Pipeline (Days 1-3)

**Anomaly Detection Script:**
```python
# ml_models/scripts/train_anomaly.py
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.covariance import EllipticEnvelope
import pandas as pd
import numpy as np
import joblib
from pathlib import Path
import json
import mlflow
import sys
sys.path.append('ml_models/scripts')
from feature_engineering import prepare_ml_data

def train_anomaly_detection(machine_id, config):
    """Train anomaly detection model (unsupervised)"""
    
    print(f"\n{'=' * 70}")
    print(f"TRAINING ANOMALY DETECTION: {machine_id}")
    print(f"{'=' * 70}\n")
    
    mlflow.set_experiment(f"ML_Anomaly_{machine_id}")
    
    with mlflow.start_run(run_name=f"{machine_id}_anomaly"):
        # Load data
        train_df, val_df, test_df = prepare_ml_data(machine_id, 'classification')
        
        # Use only normal samples for training (unsupervised)
        if 'failure_status' in train_df.columns:
            normal_data = train_df[train_df['failure_status'] == 0]
        else:
            normal_data = train_df
        
        # Remove target columns
        feature_cols = [col for col in normal_data.columns 
                       if col not in ['failure_status', 'rul', 'machine_id', 'timestamp']]
        
        X_train = normal_data[feature_cols].values
        X_test = test_df[feature_cols].values
        
        print(f"Training samples (normal): {len(X_train)}")
        print(f"Test samples: {len(X_test)}")
        print(f"Features: {len(feature_cols)}")
        
        # Train multiple anomaly detectors
        models = {}
        
        # 1. Isolation Forest (Best for high-dimensional data)
        print("\nTraining Isolation Forest...")
        iso_forest = IsolationForest(
            contamination=config.get('contamination', 0.1),
            n_estimators=config.get('n_estimators', 100),
            random_state=42,
            n_jobs=-1
        )
        iso_forest.fit(X_train)
        models['isolation_forest'] = iso_forest
        
        # 2. One-Class SVM
        print("Training One-Class SVM...")
        oc_svm = OneClassSVM(gamma='auto', nu=config.get('contamination', 0.1))
        oc_svm.fit(X_train)
        models['one_class_svm'] = oc_svm
        
        # Evaluate on test set
        results = {}
        
        if 'failure_status' in test_df.columns:
            y_true = test_df['failure_status'].values
            y_true_binary = (y_true == 1).astype(int)  # 1 for anomaly, 0 for normal
            
            for model_name, model in models.items():
                y_pred = model.predict(X_test)
                # Convert: 1 (normal) -> 0, -1 (anomaly) -> 1
                y_pred_binary = (y_pred == -1).astype(int)
                
                from sklearn.metrics import classification_report, f1_score, precision_score, recall_score
                
                metrics = {
                    'precision': precision_score(y_true_binary, y_pred_binary, zero_division=0),
                    'recall': recall_score(y_true_binary, y_pred_binary, zero_division=0),
                    'f1_score': f1_score(y_true_binary, y_pred_binary, zero_division=0)
                }
                
                results[model_name] = metrics
                
                print(f"\n{model_name.upper()} Results:")
                print(f"  Precision: {metrics['precision']:.4f}")
                print(f"  Recall: {metrics['recall']:.4f}")
                print(f"  F1 Score: {metrics['f1_score']:.4f}")
        
        # Select best model
        if results:
            best_model_name = max(results.items(), key=lambda x: x[1]['f1_score'])[0]
            best_model = models[best_model_name]
            best_metrics = results[best_model_name]
        else:
            best_model_name = 'isolation_forest'
            best_model = models[best_model_name]
            best_metrics = {}
        
        print(f"\n‚úÖ Best Model: {best_model_name}")
        
        # Save best model
        save_path = Path(f'ml_models/models/anomaly/{machine_id}')
        save_path.mkdir(parents=True, exist_ok=True)
        
        model_file = save_path / f'{best_model_name}.pkl'
        joblib.dump(best_model, model_file)
        
        # Save feature names
        feature_file = save_path / 'features.json'
        with open(feature_file, 'w') as f:
            json.dump({'features': feature_cols}, f, indent=2)
        
        # Save report
        report = {
            'machine_id': machine_id,
            'task_type': 'anomaly_detection',
            'best_model': best_model_name,
            'metrics': best_metrics,
            'all_results': results,
            'model_path': str(save_path),
            'n_features': len(feature_cols)
        }
        
        report_path = f'ml_models/reports/performance_metrics/{machine_id}_anomaly_report.json'
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"\n‚úÖ Model saved: {model_file}")
        print(f"‚úÖ Report saved: {report_path}")
        
        return report

if __name__ == "__main__":
    import argparse
    from config.model_config import AUTOGLUON_CONFIG
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--machine_id', required=True)
    args = parser.parse_args()
    
    config = AUTOGLUON_CONFIG['anomaly']
    train_anomaly_detection(args.machine_id, config)
```

**Per-Machine Anomaly Detection Training:**
```powershell
# Navigate to ml_models folder
cd ml_models

# Train anomaly model for each of 10 priority machines
python scripts/train_anomaly.py --machine_id motor_siemens_1la7_001
python scripts/train_anomaly.py --machine_id motor_abb_m3bp_002
# ... repeat for all 10 machines

# OR use batch training
python scripts/batch_train_anomaly.py --machines_file config/priority_10_machines.txt
```

**Training Details (Per Machine):**
- Input: ~40K normal samples per machine
- Features: 87 machine-specific sensor features
- Algorithms: Isolation Forest + One-Class SVM
- Training time: ~10-15 minutes per machine
- Total time: ~2.5 hours for 10 machines

**Integration with Phase 1.5 (New Machine):**
```
New Machine Added via Phase 1.5 (~2h)
       ‚Üì
Train Anomaly Model (~15min)
       ‚Üì
Total: ~2.25 hours for new machine anomaly detection
```

**Deliverables:**
- üîÑ 10 anomaly detection models (1 per priority machine)
- üîÑ Isolation Forest + One-Class SVM per machine
- üîÑ Performance metrics per machine (F1 >0.85 target)
- üîÑ Model size: ~20 MB per machine (~200 MB total)

---

## Summary of Phase 2 Part 1

### Completed Sections:
- ‚úÖ Phase 2.1: Setup & AutoML Selection (Week 1)
- ‚úÖ Phase 2.2: Classification Models (Week 2)
- ‚úÖ Phase 2.3: Regression Models (Week 3)
- ‚úÖ Phase 2.4: Anomaly Detection (Week 4)

### To Be Continued in Part 2:
- Phase 2.5: Time-Series Forecasting (Week 5)
- Phase 2.6: Edge Optimization (Week 6)
- Phase 2.7: Deployment & Documentation (Week 7)

### Current Progress:
- **60 models trained** (20 machines √ó 3 model types)
- **Remaining:** 20 time-series models + edge optimization

---

## PHASE 2.5: Time-Series Forecasting Model
**Duration:** Week 5  
**Goal:** Train ONE generic time-series forecasting model

### Phase 2.5.1: Time-Series Pipeline Setup (Days 1-3)

**Approach:** Generic LSTM/Transformer for all machines

**Script:** `ml_models/scripts/train_timeseries.py`
```python
def train_generic_timeseries_model():
    """
    Train SINGLE time-series model for ALL machines
    Uses machine_id as feature to differentiate patterns
    """
    # Create sequences from pooled data
    # Add machine metadata to each sequence
    # Train generic LSTM/Transformer
    # Predict next 24 hours for ANY machine
```

**Deliverables:**
- ‚úÖ Generic time-series preprocessing
- ‚úÖ Sequence generation with machine context
- ‚úÖ Architecture selection (LSTM vs Transformer)

---

### Phase 2.5.2: Train & Validate (Days 4-7)

**Train per-machine time-series forecasting models:**
```powershell
cd ml_models

# Train time-series model for each of 10 priority machines
python scripts/train_timeseries.py --machine_id motor_siemens_1la7_001
python scripts/train_timeseries.py --machine_id motor_abb_m3bp_002
# ... repeat for all 10 machines

# OR use batch training
python scripts/batch_train_timeseries.py --machines_file config/priority_10_machines.txt
```

**Training Details (Per Machine):**
- Input: Machine-specific time sequences
- Forecast horizon: 24 hours ahead
- Architecture: LSTM or Transformer (AutoML selection)
- Training time: ~1 hour per machine
- Total time: ~10 hours for 10 machines (sequential)

**Integration with Phase 1.5 (New Machine):**
```
New Machine Added via Phase 1.5 (~2h)
       ‚Üì
Train Time-Series Model (~1h)
       ‚Üì
Total: ~3 hours for new machine forecasting capability
```

**Deliverables:**
- üîÑ 10 time-series models (1 per priority machine)
- üîÑ MAPE <15% per machine
- üîÑ 24-hour ahead forecasting for each machine
- üîÑ Model size: ~100 MB per machine (~1 GB total)

---

## PHASE 2.6: Edge Optimization & Model Compression
**Duration:** Week 6  
**Goal:** Optimize 4 generic models for edge deployment

### Phase 2.6.1: Model Quantization (Days 1-3)

**Optimization Techniques:**
1. **ONNX Conversion** - Convert to ONNX format
2. **INT8 Quantization** - Reduce precision
3. **Pruning** - Remove unnecessary weights
4. **Knowledge Distillation** - Optional smaller models

**Script:** `ml_models/scripts/optimize_for_edge.py`
```python
def optimize_generic_model(model_path, task_type):
    """
    Optimize GENERIC model for edge deployment
    Only 4 models to optimize (not 80!)
    """
    # Load generic model
    # Convert to ONNX
    # Apply INT8 quantization
    # Validate on all machine types (>95% accuracy retained)
    # Save optimized model
```

**Models to Optimize:**
1. Classification model (50MB ‚Üí 5MB)
2. Regression model (50MB ‚Üí 5MB)
3. Anomaly model (20MB ‚Üí 2MB)
4. Time-series model (100MB ‚Üí 10MB)

**Total: 4 models (not 80!)**

**Deliverables:**
- ‚úÖ 4 optimized ONNX models
- ‚úÖ Total size: ~25 MB (all 4 models)
- ‚úÖ 90% size reduction achieved

---

### Phase 2.6.2: Edge Deployment Testing (Days 4-5)

**Testing Environments:**
1. **Raspberry Pi 4** (ARM CPU)
2. **NVIDIA Jetson Nano** (Edge GPU)
3. **Intel NUC** (x86 CPU)

**Validation Metrics:**
- Inference latency: <100ms per prediction
- Memory usage: <512 MB
- CPU usage: <50%
- Accuracy: >95% of original model

**Script:** `ml_models/scripts/test_edge_inference.py`

**Deliverables:**
- ‚úÖ Edge deployment validation
- ‚úÖ Performance benchmarks
- ‚úÖ Resource utilization reports

---

### Phase 2.6.3: Model Registry & Versioning (Days 6-7)

**Setup Model Registry:**
- MLflow Model Registry for 4 generic models
- Version control and rollback
- A/B testing setup

**Model Catalog (SIMPLIFIED):**
```
ml_models/
‚îú‚îÄ‚îÄ registry/
‚îÇ   ‚îú‚îÄ‚îÄ classification/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ v1.0_generic_original.pkl (50MB)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ v1.1_generic_quantized.onnx (5MB) ‚≠ê
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata.json
‚îÇ   ‚îú‚îÄ‚îÄ regression/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ v1.0_generic_original.pkl (50MB)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ v1.1_generic_quantized.onnx (5MB) ‚≠ê
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata.json
‚îÇ   ‚îú‚îÄ‚îÄ anomaly/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ v1.0_generic_original.pkl (20MB)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ v1.1_generic_quantized.onnx (2MB) ‚≠ê
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata.json
‚îÇ   ‚îî‚îÄ‚îÄ timeseries/
‚îÇ       ‚îú‚îÄ‚îÄ v1.0_generic_original.pkl (100MB)
‚îÇ       ‚îú‚îÄ‚îÄ v1.1_generic_quantized.onnx (10MB) ‚≠ê
‚îÇ       ‚îî‚îÄ‚îÄ metadata.json
```

**Deliverables:**
- ‚úÖ 4 generic models registered (not 80!)
- ‚úÖ Optimized versions ready for deployment
- ‚úÖ Metadata tracking for all versions

---

## PHASE 2.7: Deployment, API & Documentation
**Duration:** Week 7  
**Goal:** Create production-ready deployment infrastructure

### Phase 2.7.1: REST API Development (Days 1-3)

**API Framework:** FastAPI with GENERIC models

**Endpoints:**
- `POST /predict/classification` - Failure prediction (any machine)
- `POST /predict/rul` - RUL estimation (any machine)
- `POST /predict/anomaly` - Anomaly detection (any machine)
- `POST /predict/forecast` - Time-series forecasting (any machine)
- `GET /models/info` - Model info
- `GET /health` - Health check

**Script:** `ml_models/api/main.py`
```python
# FastAPI with GENERIC models (scalable approach)
from fastapi import FastAPI

# Load 4 generic models at startup (not 80 models!)
classification_model = load_onnx_model('registry/classification/v1.1_generic_quantized.onnx')
regression_model = load_onnx_model('registry/regression/v1.1_generic_quantized.onnx')
anomaly_model = load_onnx_model('registry/anomaly/v1.1_generic_quantized.onnx')
timeseries_model = load_onnx_model('registry/timeseries/v1.1_generic_quantized.onnx')

@app.post("/predict/classification")
async def predict_failure(machine_id: str, sensor_data: dict):
    """
    Works for ANY machine (including new ones!)
    Just needs machine_id + sensor_data
    """
    # Add machine metadata features
    features = add_machine_metadata(sensor_data, machine_id)
    # Single model handles all machines
    prediction = classification_model.predict(features)
    return {"machine_id": machine_id, "failure_probability": prediction}
```

**Key Advantage:**
- ‚úÖ API loads only 4 models (not 80!)
- ‚úÖ Memory efficient: ~25 MB total
- ‚úÖ New machine = works immediately (no model update needed!)
- ‚úÖ Simpler deployment and maintenance

**Features:**
- Request validation (Pydantic models)
- Error handling and logging
- Rate limiting
- Authentication (API keys)
- Response caching

**Deliverables:**
- ‚úÖ FastAPI application
- ‚úÖ API documentation (auto-generated)
- ‚úÖ Docker containerization
- ‚úÖ Load testing (1000+ requests/sec)

---

### Phase 2.7.2: Monitoring & Logging (Days 4-5)

**Monitoring Stack:**
1. **Prometheus** - Metrics collection
2. **Grafana** - Dashboards
3. **ELK Stack** - Log aggregation (optional)

**Metrics to Track:**
- Inference latency (p50, p95, p99)
- Model accuracy drift
- Request throughput
- Error rates
- Resource utilization (CPU, memory, GPU)

**Script:** `ml_models/monitoring/setup_monitoring.py`

**Dashboards:**
- Real-time inference metrics
- Model performance comparison
- System health monitoring
- Prediction distribution analysis

**Deliverables:**
- ‚úÖ Prometheus + Grafana setup
- ‚úÖ Custom dashboards (5-10)
- ‚úÖ Alerting rules (performance degradation)
- ‚úÖ Logging infrastructure

---

### Phase 2.7.3: Documentation & Handoff (Days 6-7)

**Documentation Deliverables:**

**1. Technical Documentation:**
```markdown
# ML Models Documentation

## Model Architecture
- Classification: AutoGluon ensemble (RF, XGBoost, LightGBM)
- Regression: Gradient Boosting + Neural Networks
- Anomaly: Isolation Forest + One-Class SVM
- Time-Series: LSTM + Transformer

## Performance Metrics
- Classification: F1 >0.90 (all machines)
- Regression: R¬≤ >0.75 (all machines)
- Anomaly: F1 >0.85 (all machines)
- Time-Series: MAPE <15% (all machines)

## API Usage
- Endpoint documentation
- Request/response examples
- Authentication guide
- Rate limits and quotas
```

**2. Deployment Guide:**
- Docker deployment instructions
- Kubernetes configuration (optional)
- Environment variables
- Scaling guidelines

**3. Model Cards:**
Create model card for each machine with:
- Model type and version
- Training data summary
- Performance metrics
- Known limitations
- Maintenance recommendations

**4. User Guide:**
- How to query predictions
- Interpreting results
- Troubleshooting common issues
- Feature importance explanations

**5. Phase 2 Completion Report:**
```markdown
# Phase 2 Completion Report

## Summary
- Duration: 7 weeks
- Machines: 20 (current) + unlimited (scalable)
- Total Models: **4 GENERIC models** (not 80!)
- Model Types: Classification, Regression (RUL), Anomaly, Time-Series

## Results
- Classification F1: >0.90 (across all machines)
- Regression R¬≤: >0.75 (across all machines)
- Anomaly F1: >0.85 (across all machines)
- Time-Series MAPE: <15% (across all machines)

## Deliverables
- ‚úÖ 4 generic ML models (works for ALL machines)
- ‚úÖ 4 edge-optimized models (ONNX, total ~25 MB)
- ‚úÖ REST API (loads only 4 models)
- ‚úÖ Monitoring dashboards
- ‚úÖ Complete documentation
- ‚úÖ MLflow tracking and registry

## Model Storage (OPTIMIZED)
- Original models: ~220 MB total (4 models)
- Optimized models: ~25 MB total (4 models)
- 90% size reduction achieved
- **API memory footprint: Only 25 MB!**

## Scalability Advantage
- ‚úÖ **Adding new machine:** Just generate GAN data ‚Üí Works immediately!
- ‚úÖ **No retraining needed** for similar machine types
- ‚úÖ **4 models to maintain** (not 80!)
- ‚úÖ **Single API deployment** handles all machines

## Performance
- Inference latency: <50ms (avg)
- API throughput: 1000+ req/sec
- Edge deployment: Validated on 3 platforms

## Next Steps
- Deploy to production edge devices
- Integrate with Phase 3: LLM explanations (optional)
- Continuous monitoring and retraining pipeline
```

**Deliverables:**
- ‚úÖ Complete technical documentation
- ‚úÖ API documentation (Swagger/OpenAPI)
- ‚úÖ Deployment guides
- ‚úÖ Model cards (20 machines)
- ‚úÖ Phase 2 completion report
- ‚úÖ Lessons learned document
- ‚úÖ Ready for production deployment

---

## Phase 2 Summary

### Timeline (7 Weeks)
- **Week 1:** Setup & AutoML Selection
- **Week 2:** Classification Models (10 machines)
- **Week 3:** Regression Models (10 machines)
- **Week 4:** Anomaly Detection (10 machines)
- **Week 5:** Time-Series Forecasting (10 machines)
- **Week 6:** Edge Optimization & Model Registry
- **Week 7:** Deployment, API & Documentation

### Key Deliverables
- ‚úÖ **40 Per-Machine ML models** (10 machines √ó 4 model types)
  - 10 Classification models (1 per priority machine)
  - 10 Regression models (1 per priority machine)
  - 10 Anomaly detection models (1 per priority machine)
  - 10 Time-series forecasting models (1 per priority machine)
- üîÑ 40 edge-optimized models (ONNX, quantized)
- üîÑ REST API (FastAPI, model routing per machine)
- üîÑ Monitoring infrastructure (Prometheus + Grafana)
- üîÑ Complete documentation
- üîÑ Docker deployment ready
- ‚úÖ **Phase 1.5 Integration:** New machine workflow documented

### Performance Metrics (Targets)
- üéØ Classification F1: >0.85 per machine (better than generic)
- üéØ Regression R¬≤: >0.75 per machine
- üéØ Anomaly F1: >0.85 per machine
- üéØ Time-Series MAPE: <15% per machine
- üéØ Inference latency: <100ms per prediction
- üéØ Total model size: ~2.2 GB (40 models, ~50-100 MB each)

### Success Metrics
- üîÑ 40 per-machine models (10 machines √ó 4 types)
- üîÑ Edge optimization: 90% size reduction per model
- üîÑ API performance: machine-specific routing
- üîÑ Production-ready deployment
- ‚úÖ **Scalability via Phase 1.5:** New machine = 6 hours (Phase 1.5: 2h + Phase 2: 4h)
- üîÑ Comprehensive monitoring and logging

---

## Future Scope & Production Enhancements

### Model Performance Improvements

#### 1. **Real Data Fine-Tuning** (Priority: CRITICAL)
**Current:** Models trained on 100% synthetic data  
**Target:** Hybrid training with real sensor data

**Implementation Strategy:**
```python
# Phase 2.8: Real Data Integration
# Step 1: Collect 1-3 months real sensor data from production
# Step 2: Label real failures (if any occurred)
# Step 3: Fine-tune models with transfer learning:
#   - Start with synthetic-trained model
#   - Continue training on small real dataset
#   - Validation on held-out real data
```

**Expected Impact:**
- Classification F1: 0.85 ‚Üí 0.92+ (real data)
- Regression R¬≤: 0.75 ‚Üí 0.85+ (real data)
- Reduce false positive rate by 30-50%
- Better generalization to production conditions

**Challenges:**
- Real failures are rare (need 6-12 months data)
- Data quality issues (sensor drift, missing values)
- Labeling cost (domain expertise required)

**Timeline:** 3-6 months (data collection + retraining)

---

#### 2. **Continuous Learning Pipeline** (Priority: HIGH)
**Current:** Static models (trained once)  
**Target:** Self-improving models with production feedback

**Architecture:**
```
Production Deployment
       ‚Üì
Collect Predictions + Outcomes
       ‚Üì
Detect Model Drift (weekly)
       ‚Üì
Retrain if Performance Drops >10%
       ‚Üì
A/B Test New Model vs Old
       ‚Üì
Promote if Better Performance
       ‚Üì
Automatic Model Update
```

**Implementation:**
```python
# Phase 2.9: MLOps Pipeline
# - Data versioning (DVC)
# - Model versioning (MLflow)
# - Automated retraining (Airflow/Kubeflow)
# - A/B testing framework
# - Drift detection (Evidently AI)
# - Rollback mechanism
```

**Benefits:**
- Models adapt to changing equipment conditions
- Automatic quality improvement over time
- Detect and fix degradation early
- Reduced manual intervention

**Timeline:** 4-6 weeks development + 2 weeks testing

---

#### 3. **Ensemble & Stacking Strategies** (Priority: MEDIUM)
**Current:** Single best model per machine/task  
**Target:** Intelligent model ensembles

**Approach:**
```python
# Multi-level ensemble:
# Level 1: Train 5 diverse models per machine
#   - XGBoost (tree-based)
#   - LightGBM (fast tree)
#   - CatBoost (categorical)
#   - Neural Network (deep learning)
#   - Random Forest (robust)
# Level 2: Meta-learner combines predictions
#   - Learns when each model is most reliable
#   - Weighted voting based on confidence
```

**Expected Improvement:**
- Classification F1: +3-5% boost
- More robust to edge cases
- Reduced variance in predictions

**Trade-off:** 5√ó longer training time, 5√ó storage

**Timeline:** 2-3 weeks implementation

---

#### 4. **Explainable AI (XAI)** (Priority: HIGH)
**Current:** Black-box predictions  
**Target:** Interpretable predictions with explanations

**Implementation:**
```python
# Phase 2.10: Explainability Layer
# Technique 1: SHAP (SHapley Additive exPlanations)
#   - Feature importance per prediction
#   - "Pump bearing temp (85¬∞C) contributed 60% to failure prediction"

# Technique 2: LIME (Local Interpretable Model-Agnostic)
#   - Approximate model locally with simple rules
#   - "If temp > 80¬∞C AND vibration > 6 mm/s ‚Üí 85% failure risk"

# Technique 3: Attention Visualization (for time-series)
#   - Highlight which time windows influenced prediction
```

**Benefits:**
- Build trust with maintenance teams
- Identify root causes faster
- Regulatory compliance (explainability required)
- Debug model errors easier

**API Enhancement:**
```json
// Prediction response with explanation
{
  "machine_id": "motor_siemens_1la7_001",
  "prediction": "failure",
  "probability": 0.87,
  "explanation": {
    "top_factors": [
      {"feature": "winding_temp_C", "value": 142, "contribution": 0.45},
      {"feature": "bearing_vibration_mm_s", "value": 8.2, "contribution": 0.32},
      {"feature": "current_imbalance_pct", "value": 15, "contribution": 0.23}
    ],
    "recommendation": "Inspect motor bearings and check winding insulation"
  }
}
```

**Timeline:** 3-4 weeks development

---

### Edge Deployment Enhancements

#### 5. **Multi-Platform Optimization** (Priority: MEDIUM)
**Current:** ONNX for general edge devices  
**Target:** Platform-specific optimizations

**Platforms:**
```python
# ARM-based (Raspberry Pi, Jetson):
#   - TensorFlow Lite
#   - INT8 quantization
#   - Target: <5 MB per model, <50ms latency

# FPGA (Industrial controllers):
#   - Vitis AI optimization
#   - Ultra-low latency (<10ms)
#   - High throughput (1000+ predictions/sec)

# Mobile (Android/iOS for field maintenance):
#   - Core ML (iOS)
#   - TensorFlow Lite (Android)
#   - On-device inference, no internet needed
```

**Timeline:** 2-3 weeks per platform

---

#### 6. **Federated Learning** (Priority: LOW)
**Current:** Centralized training  
**Target:** Train on edge devices without data centralization

**Use Case:**
- Each factory has 10-20 machines
- Privacy concerns (can't share raw sensor data)
- Train models locally, share only model updates
- Central server aggregates improvements

**Timeline:** 6-8 weeks (complex implementation)

---

### Scalability & Performance

#### 7. **Distributed Training** (Priority: MEDIUM)
**Current:** Sequential training (25 hours for 10 machines)  
**Target:** Parallel training across multiple GPUs/machines

**Architecture:**
```python
# Phase 2.11: Distributed Training
# Setup: 4 GPUs (or 4 cloud instances)
# 
# GPU 1: Trains machines 1-3 (Classification + Regression)
# GPU 2: Trains machines 4-6 (Classification + Regression)
# GPU 3: Trains machines 7-9 (Classification + Regression)
# GPU 4: Trains machine 10 + All Anomaly models
#
# Time reduction: 25 hours ‚Üí 6-7 hours
```

**Tools:**
- Ray Tune (distributed hyperparameter tuning)
- Horovod (multi-GPU training)
- Kubernetes (cloud orchestration)

**Timeline:** 2-3 weeks setup + testing

---

#### 8. **AutoML Pipeline** (Priority: MEDIUM)
**Current:** Manual AutoGluon configuration  
**Target:** Fully automated hyperparameter optimization

**Implementation:**
```python
# Phase 2.12: Advanced AutoML
# - Neural Architecture Search (NAS)
# - Automated feature engineering
# - Meta-learning (learn from past trainings)
# - Transfer learning from similar machines
# - Multi-objective optimization (accuracy + speed + size)
```

**Expected Benefits:**
- 5-10% better performance per model
- No manual tuning required
- Consistent quality across all machines

**Timeline:** 3-4 weeks

---

### Production & Operations

#### 9. **Advanced Monitoring Dashboard** (Priority: HIGH)
**Current:** Basic Prometheus + Grafana  
**Target:** Comprehensive ML observability

**Dashboard Components:**
```
1. Model Performance Metrics
   - Per-machine accuracy, precision, recall
   - Trend analysis (degradation detection)
   - Confusion matrices (updated hourly)

2. Prediction Analytics
   - Failure prediction rate per machine
   - False positive/negative tracking
   - Prediction confidence distribution

3. Business KPIs
   - Maintenance cost reduction
   - Unplanned downtime prevented
   - ROI tracking

4. Data Quality
   - Sensor drift detection
   - Missing data alerts
   - Outlier frequency

5. System Health
   - API latency (p50, p95, p99)
   - Model inference time
   - Error rates and types
```

**Implementation:**
- Custom Grafana dashboards
- Integration with business intelligence tools
- Automated alerting (PagerDuty, Slack)
- Weekly executive reports (auto-generated)

**Timeline:** 3-4 weeks development

---

#### 10. **Multi-Tenancy Support** (Priority: MEDIUM)
**Current:** Single deployment for one organization  
**Target:** SaaS platform for multiple clients

**Architecture:**
```python
# Phase 2.13: Multi-Tenant Platform
# - Tenant isolation (separate models per client)
# - Resource quotas (API rate limits per tenant)
# - Custom branding per client
# - Pay-per-prediction billing
# - White-label API
```

**Business Model:**
- Tier 1: $500/month - 10 machines, 10K predictions
- Tier 2: $2000/month - 50 machines, 100K predictions
- Enterprise: Custom pricing

**Timeline:** 6-8 weeks development

---

#### 11. **Mobile Maintenance App** (Priority: LOW)
**Current:** API only (for integrations)  
**Target:** Native mobile app for maintenance teams

**Features:**
```
- Real-time machine health dashboard
- Push notifications for predicted failures
- Offline mode (cached predictions)
- Maintenance checklist guided by AI
- Photo upload for damage assessment
- Work order integration
- Technician performance tracking
```

**Platforms:** iOS + Android  
**Timeline:** 8-12 weeks development

---

### Advanced Features

#### 12. **Prescriptive Maintenance** (Priority: MEDIUM)
**Current:** Predictive ("failure in 7 days")  
**Target:** Prescriptive ("replace bearing now, saves $5000")

**Implementation:**
```python
# Phase 2.14: Optimization Engine
# Input:
#   - Failure predictions from ML models
#   - Maintenance costs (labor, parts, downtime)
#   - Production schedule
#   - Spare parts inventory
#
# Output:
#   - Optimal maintenance schedule
#   - Cost-benefit analysis
#   - Resource allocation
#
# Algorithm: Mixed Integer Programming (MIP)
#   - Minimize total cost
#   - Constraints: available technicians, budget, parts
#   - Prioritize critical machines
```

**Expected Impact:**
- 20-30% maintenance cost reduction
- Optimal resource utilization
- Reduced emergency repairs

**Timeline:** 4-6 weeks (requires operations research expertise)

---

#### 13. **Digital Twin Integration** (Priority: LOW)
**Current:** Standalone ML models  
**Target:** Integration with physics-based digital twins

**Hybrid Approach:**
```python
# Combine data-driven ML + physics simulation
# ML: Learns from patterns (anomaly detection)
# Physics: Models equipment behavior (thermodynamics, mechanics)
# Hybrid: Best of both worlds (accuracy + interpretability)
```

**Timeline:** 3-6 months (requires physics modeling expertise)

---

#### 14. **Natural Language Interface** (Priority: LOW)
**Current:** JSON API  
**Target:** Ask questions in plain English

**Examples:**
```
User: "Which motors are at risk this week?"
AI: "3 motors show elevated failure risk: 
     - Motor Siemens 001: 75% risk, high bearing temp
     - Motor ABB 002: 60% risk, vibration anomaly
     - Motor WEG 003: 55% risk, current imbalance"

User: "What maintenance should I prioritize today?"
AI: "Top priority: Inspect Motor Siemens 001 bearing 
     (predicted failure in 3 days, $8,000 downtime cost)"
```

**Implementation:**
- LLM integration (GPT-4 API)
- Vector database for context (Pinecone/Weaviate)
- Voice interface (optional)

**Timeline:** 3-4 weeks

---

### Testing & Quality Assurance

#### 15. **Automated Testing Suite** (Priority: HIGH)
**Current:** Manual validation  
**Target:** Comprehensive automated testing

**Test Coverage:**
```python
# Unit Tests (per model):
#   - Input validation
#   - Output shape correctness
#   - Edge cases (missing sensors, outliers)

# Integration Tests (API):
#   - End-to-end prediction flow
#   - Error handling
#   - Load testing (1000+ concurrent requests)

# Performance Tests:
#   - Latency benchmarks
#   - Memory leak detection
#   - GPU utilization

# Model Quality Tests:
#   - Accuracy regression (alert if drops >5%)
#   - Prediction consistency
#   - Bias detection
```

**CI/CD Pipeline:**
```yaml
# .github/workflows/ml_pipeline.yml
- Run on every commit
- Automated model training on test data
- Performance validation
- Auto-deploy if all tests pass
```

**Timeline:** 2-3 weeks

---

### Documentation & Training

#### 16. **Interactive Documentation** (Priority: MEDIUM)
**Current:** Static markdown files  
**Target:** Interactive docs with live examples

**Features:**
```
- API playground (test predictions in browser)
- Interactive tutorials (Jupyter notebooks)
- Video walkthroughs
- FAQ chatbot
- Code generators (Python, JavaScript, cURL)
```

**Tools:** Docusaurus, Swagger UI, Postman  
**Timeline:** 2-3 weeks

---

#### 17. **Training Program for Maintenance Teams** (Priority: HIGH)
**Content:**
```
1. Understanding AI Predictions (2 hours)
   - How models work (simplified)
   - Reading confidence scores
   - When to trust vs verify predictions

2. Using the System (3 hours)
   - Dashboard walkthrough
   - Interpreting alerts
   - Logging feedback (improve models)

3. Troubleshooting (2 hours)
   - Common issues and solutions
   - When to escalate
   - Emergency procedures
```

**Timeline:** 2 weeks course development + ongoing training

---

## Implementation Roadmap

### Phase 2.8: Production Readiness (Weeks 8-10)
**Priority: CRITICAL**
- [ ] Real data collection pipeline (Week 8)
- [ ] Fine-tune models with real data (Week 9)
- [ ] Deploy to staging environment (Week 10)
- [ ] Load testing and validation (Week 10)

### Phase 2.9: MLOps & Automation (Weeks 11-13)
**Priority: HIGH**
- [ ] Continuous learning pipeline (Week 11-12)
- [ ] Advanced monitoring dashboard (Week 12)
- [ ] Automated testing suite (Week 13)
- [ ] A/B testing framework (Week 13)

### Phase 2.10: Enhanced Features (Weeks 14-17)
**Priority: MEDIUM**
- [ ] Explainable AI integration (Week 14-15)
- [ ] Prescriptive maintenance engine (Week 16-17)
- [ ] Mobile app development (Week 14-17, parallel)

### Phase 2.11: Scale & Optimize (Weeks 18-20)
**Priority: MEDIUM**
- [ ] Distributed training setup (Week 18)
- [ ] Multi-platform edge optimization (Week 19)
- [ ] Multi-tenancy support (Week 20)

### Phase 2.12: Advanced R&D (Months 6-12)
**Priority: LOW (Long-term)**
- [ ] Digital twin integration
- [ ] Federated learning
- [ ] Natural language interface
- [ ] Advanced ensemble methods

---

## Success Metrics (12-Month Goals)

**Model Performance:**
- ‚úÖ Classification F1 >0.92 (real data)
- ‚úÖ Regression R¬≤ >0.85 (real data)
- ‚úÖ False positive rate <5%
- ‚úÖ Prediction confidence calibration >90%

**System Performance:**
- ‚úÖ API latency <50ms (p95)
- ‚úÖ 99.9% uptime SLA
- ‚úÖ Handle 10,000+ predictions/sec
- ‚úÖ Support 100+ machines across 5+ clients

**Business Impact:**
- ‚úÖ Reduce unplanned downtime by 40%
- ‚úÖ Cut maintenance costs by 25%
- ‚úÖ ROI >300% within 18 months
- ‚úÖ Prevent 2+ critical failures per client/month

**User Adoption:**
- ‚úÖ 90%+ maintenance team trained
- ‚úÖ 80%+ prediction feedback rate
- ‚úÖ <5 min average time to investigate alert
- ‚úÖ 95%+ user satisfaction score

---

## Next Actions

### Immediate (This Week):
1. ‚úÖ Complete Phase 2.2-2.5 training (40 models for 10 machines)
2. ‚úÖ Validate model performance (all F1 >0.85)
3. ‚úÖ Begin edge optimization (Phase 2.6)

### Short-term (Next 2-4 Weeks):
4. Complete Phase 2.6-2.7 (optimization + deployment)
5. Set up monitoring infrastructure
6. Deploy to staging environment
7. Begin real data collection planning

### Medium-term (Next 1-3 Months):
8. Implement Phase 2.8 (real data fine-tuning)
9. Build MLOps pipeline (Phase 2.9)
10. Add explainability features (Phase 2.10)
11. Scale to 20+ machines

### Long-term (Next 6-12 Months):
12. Advanced features (prescriptive maintenance)
13. Multi-tenancy platform
14. Mobile app launch
15. Scale to 100+ machines, 5+ clients

**Current Status:** Phase 2.2 in progress ‚Üí Proceeding with per-machine model training

### Files Generated
```
ml_models/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ model_config.py
‚îÇ   ‚îî‚îÄ‚îÄ training_config.json
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ processed/              (symlink to GAN data)
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ classification/         (20 models)
‚îÇ   ‚îú‚îÄ‚îÄ regression/             (20 models)
‚îÇ   ‚îú‚îÄ‚îÄ anomaly/               (20 models)
‚îÇ   ‚îî‚îÄ‚îÄ timeseries/            (20 models)
‚îú‚îÄ‚îÄ registry/
‚îÇ   ‚îú‚îÄ‚îÄ classification/         (20 optimized models)
‚îÇ   ‚îú‚îÄ‚îÄ regression/            (20 optimized models)
‚îÇ   ‚îú‚îÄ‚îÄ anomaly/              (20 optimized models)
‚îÇ   ‚îî‚îÄ‚îÄ timeseries/           (20 optimized models)
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                (FastAPI application)
‚îÇ   ‚îú‚îÄ‚îÄ models.py              (Pydantic schemas)
‚îÇ   ‚îú‚îÄ‚îÄ inference.py           (Inference logic)
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ monitoring/
‚îÇ   ‚îú‚îÄ‚îÄ prometheus.yml
‚îÇ   ‚îú‚îÄ‚îÄ grafana_dashboards/
‚îÇ   ‚îî‚îÄ‚îÄ alerting_rules.yml
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ train_classification.py
‚îÇ   ‚îú‚îÄ‚îÄ train_regression.py
‚îÇ   ‚îú‚îÄ‚îÄ train_anomaly.py
‚îÇ   ‚îú‚îÄ‚îÄ train_timeseries.py
‚îÇ   ‚îú‚îÄ‚îÄ batch_train_classification.py
‚îÇ   ‚îú‚îÄ‚îÄ batch_train_regression.py
‚îÇ   ‚îú‚îÄ‚îÄ batch_train_anomaly.py
‚îÇ   ‚îú‚îÄ‚îÄ batch_train_timeseries.py
‚îÇ   ‚îú‚îÄ‚îÄ optimize_for_edge.py
‚îÇ   ‚îú‚îÄ‚îÄ test_edge_inference.py
‚îÇ   ‚îú‚îÄ‚îÄ feature_engineering.py
‚îÇ   ‚îî‚îÄ‚îÄ verify_phase1_data.py
‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îú‚îÄ‚îÄ training_logs/
‚îÇ   ‚îú‚îÄ‚îÄ performance_metrics/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ {machine_id}_classification_report.json (√ó20)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ {machine_id}_regression_report.json (√ó20)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ {machine_id}_anomaly_report.json (√ó20)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ {machine_id}_timeseries_report.json (√ó20)
‚îÇ   ‚îú‚îÄ‚îÄ comparison_reports/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classification_comparison.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ regression_comparison.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anomaly_comparison.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ timeseries_comparison.csv
‚îÇ   ‚îú‚îÄ‚îÄ data_verification_report.csv
‚îÇ   ‚îú‚îÄ‚îÄ batch_classification_training_summary.json
‚îÇ   ‚îú‚îÄ‚îÄ batch_regression_training_summary.json
‚îÇ   ‚îú‚îÄ‚îÄ batch_anomaly_training_summary.json
‚îÇ   ‚îú‚îÄ‚îÄ batch_timeseries_training_summary.json
‚îÇ   ‚îú‚îÄ‚îÄ edge_optimization_report.json
‚îÇ   ‚îî‚îÄ‚îÄ phase_2_completion_report.md
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ exploratory_analysis.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ model_comparison.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ feature_importance_analysis.ipynb
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ API_DOCUMENTATION.md
‚îÇ   ‚îú‚îÄ‚îÄ DEPLOYMENT_GUIDE.md
‚îÇ   ‚îú‚îÄ‚îÄ MODEL_CARDS/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ {machine_id}_model_card.md (√ó20)
‚îÇ   ‚îú‚îÄ‚îÄ USER_GUIDE.md
‚îÇ   ‚îî‚îÄ‚îÄ TECHNICAL_DOCUMENTATION.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ PHASE_2_ML_DETAILED_APPROACH.md
‚îî‚îÄ‚îÄ README.md
```

### Storage & Resources
- **Original Models:** ~8 GB (80 models √ó ~100 MB avg)
- **Optimized Models:** ~800 MB (80 models √ó ~10 MB)
- **Total Reports:** ~5 MB
- **Training Time:** ~280 hours total (35 hours per week √ó 7 weeks, parallelized)
- **API Deployment:** Docker container (~2 GB)

---

## Integration with Phase 1 (GAN)

**Data Flow:**
```
Phase 1 (GAN)                    Phase 2 (ML)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Synthetic Data                   ML Training
(1.05M samples)   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>  (4 generic models)
     ‚îÇ                                ‚îÇ
     ‚îÇ                                ‚îÇ
     v                                v
GAN/data/synthetic/          ml_models/models/
‚îú‚îÄ‚îÄ motor_siemens.../        ‚îú‚îÄ‚îÄ classification/
‚îÇ   ‚îú‚îÄ‚îÄ train.parquet        ‚îÇ   ‚îî‚îÄ‚îÄ generic_all_machines/
‚îÇ   ‚îú‚îÄ‚îÄ val.parquet          ‚îú‚îÄ‚îÄ regression/
‚îÇ   ‚îî‚îÄ‚îÄ test.parquet         ‚îÇ   ‚îî‚îÄ‚îÄ generic_all_machines/
‚îú‚îÄ‚îÄ pump_grundfos.../        ‚îú‚îÄ‚îÄ anomaly/
‚îú‚îÄ‚îÄ ... (21 machines)        ‚îÇ   ‚îî‚îÄ‚îÄ generic_all_machines/
                             ‚îî‚îÄ‚îÄ timeseries/
                                 ‚îî‚îÄ‚îÄ generic_all_machines/
```

**Key Connection Points:**
1. ML models use pooled synthetic data from all 21 machines in Phase 1
2. Feature engineering maintains compatibility with GAN outputs + adds machine metadata
3. Validation uses test splits from Phase 1 (per-machine and overall)
4. Quality metrics compare synthetic vs real performance expectations

**Phase 1.5 Integration (New Machine Addition):**
```
New Machine Request
       ‚Üì
Phase 1.5: Add Metadata & Train TVAE
  - Create: GAN/metadata/new_machine_xyz.json
  - Train TVAE: ~2 hours
  - Generate: 50K synthetic samples
       ‚Üì
Phase 2: Generic Models Work Immediately!
  - Classification: Predict immediately (0h)
  - Regression: Predict immediately (0h)
  - Anomaly: Predict immediately (0h)
  - Time-series: Predict immediately (0h)
       ‚Üì
Optional: Fine-tune if needed
  - Only if new machine category very different
  - Otherwise, transfer learning handles it
```

**Scalability Advantages:**
- ‚úÖ Phase 1.5 adds new machine ‚Üí Phase 2 models work without retraining
- ‚úÖ Machine metadata (category, manufacturer, power) enables generalization
- ‚úÖ 4 generic models handle unlimited machines (not 4 √ó N models)
- ‚úÖ New machine deployment time: Phase 1.5 (2h) + Phase 2 (0h) = 2 hours total

---

## Next Phase Options

**Phase 3: LLM Integration (Optional, Cloud-only)**
- Natural language explanations
- Root cause analysis
- Maintenance recommendations
- Report generation

**Phase 4: VLM Integration (Optional, if cameras available)**
- Visual inspection
- Thermal image analysis
- Defect detection
- Equipment condition assessment

**Phase 5: MLOps & Production (Recommended)**
- CI/CD pipelines
- Automated retraining
- A/B testing framework
- Data drift monitoring
- Model performance tracking
- Incident response automation

---

**üéâ Phase 2 Complete! 80 production-ready ML models trained and deployed!**

**Next Steps:**
1. Deploy models to edge devices
2. Monitor performance in production
3. Set up automated retraining pipeline
4. (Optional) Proceed to Phase 3: LLM explanations
5. (Optional) Proceed to Phase 5: MLOps automation
